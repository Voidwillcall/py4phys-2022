{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise sheet 5\n",
    "### Due 02/12/2022, 14:00 \n",
    "- Name 1\n",
    "- Name 2\n",
    "- Name 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun with files: bright astronomical transients\n",
    "The *Bright Transient Survey* (BTS) is an astronomical survey carried out by the Zwicky Transient Facility (ZTF), a multi-telescope research infrastructure targeting transient astronomical phenomena. The BTS website provides a catalogue of very bright transient astronomy sources, such as supernovae, tidal disruption events and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the current version of the catalogue\n",
    "- Open the [BTS explorer](https://sites.astro.caltech.edu/ztf/bts/explorer.php) webpage. Keep all the default options and select \"Display as: CSV\".\n",
    "- Download the webpage as a CSV file and place it at `\"./data/bts.csv\"` where `.` is the \"current working directory\" of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\"\"\"\n",
    "Define the path to the catalogue in a way that is relative to the current working directory of the notebook.\n",
    "Remember that your code will be executed in another computer and all \"absolute\" paths (for example containing your username or system) will not work! \n",
    "\"\"\"\n",
    "base_dir = Path(os.getcwd())\n",
    "data_dir = base_dir / \"data\"\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "filename = \"bts.csv\"\n",
    "# Let's put bts.csv in the designated directory.\n",
    "catalogue_path = data_dir / \"bts.csv\"\n",
    "\n",
    "catalogue_path.is_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read the file\n",
    "- Define a function to read the file making use of the [CSV library](https://docs.python.org/3/library/csv.html).\n",
    "- The function should return a dictionary mapping a field name to a list, each list should contain the corresponding field values for all the entries in the file. In other words, `dataset[\"redshift\"][i]` should give the \"redshift\" value of the i-th entry in the CSV file.\n",
    "- Notice that the first line of the CSV file contains the field names. These are what we call *metadata*, information that *describe the data*. Use these names as keys of the dictionary.\n",
    "- Make sure that numerical values are actually stored at numbers and not as strings. You can keep \"RA\" and \"Dec\" as strings since we do not know how to deal with date/times/coordinates (yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_bts(file_path : str):\n",
    "    with open(file_path) as f:\n",
    "        for row in f:\n",
    "            if row.isspace():\n",
    "                # this allows us to skip empty lines, if any\n",
    "                continue\n",
    "            else:\n",
    "                # we derive the list of fields, and sanitize them with \"strip\" to remove \\n where present\n",
    "                fieldnames = [field.strip() for field in row.split(',')]\n",
    "                break\n",
    "        print(f\"Parsed CSV with fields:  {fieldnames}\")\n",
    "\n",
    "        data = { field : list() for field in fieldnames}\n",
    "\n",
    "        reader = csv.DictReader(f, fieldnames=fieldnames)\n",
    "        for r in reader:\n",
    "            for field in fieldnames:\n",
    "                data[field].append(r[field])\n",
    "\n",
    "        \"\"\"\n",
    "        Let's now convert the numeric fields to float.\n",
    "        \"\"\"\n",
    "\n",
    "        numeric_fields = ['peakt', 'peakmag', 'peakabs', 'duration', 'rise', 'fade', 'redshift','b','A_V']\n",
    "\n",
    "        \"\"\"\n",
    "        We may have different cases where the value is not convertible, we can try to address these case by case until our code does not throw any more errors:\n",
    "        - missing values are marked with '-' and converted to None (in the future, with numpy, this could be a NaN);\n",
    "        - lower or upper limits '>', '<' can be treated in different ways, either converted to `None` or taking the limit as the numeric value itself. We can argue that upper/lower limits are not really \"homogeneous\" to actual measurements, so discarding them is the most robust choice.\n",
    "        \"\"\"\n",
    "        for field in numeric_fields:\n",
    "            column = data[field]\n",
    "            numeric_column = list()\n",
    "            for val in column:\n",
    "                if val in [\"-\", \"\"] or val.startswith(('>','<')):\n",
    "                    num_val = None\n",
    "                else:\n",
    "                    num_val = float(val)\n",
    "                numeric_column.append(num_val)\n",
    "            data[field] = numeric_column\n",
    "        \n",
    "    return data\n",
    "\n",
    "dataset = read_bts(catalogue_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Catalogue as a class\n",
    "Define a class `Catalogue` that is meant **store the catalogue** and provide a set of utility and analysis methods according to the following:\n",
    "- the  `__init__()` method of the class should take as an argument the dictionary as defined at point `#2`;\n",
    "- provide the class with a `from_csv()` class method that takes as an argument a CSV file path and returns an instance of `Catalogue` representing the file content. You may recycle the code of your `read_bts()` function but make sure all the necessary code is contained in the class (in other words, do not call `read_bts()` from inside the class);\n",
    "- provide the `Catalogue` class with a non-modifiable `origin` attribute (to be taken care of in `__init__()`). `origin` should tell whether the object has been created from a dictionary, another catalogue (see later `filter_by_types()`), or a file. In the case of a file, `origin` should keep track of the file path in the form of a string;\n",
    "- provide the class with a `__repr__()` method. Remember: this method is meant to return a string representation of the object but in general not of its full content. Use `__repr__()` method to return a string containing at least the size of the catalogue and the `origin` information;\n",
    "- provide the class with a `__str__()` format. Remember: this method is meant to return a more descriptive representation than `__repr__()` that is used when the instance is passed to the `print()` function. The `__str__()` method should return a string with a table-like representation of the catalogue content, limited to the following fields: `IAUID`, `RA`, `Dec`, `peakabs`, `type`, `redshift`. Since it may not be always desiderable to print a long catalogue, make `__str()__` depend on an instance attribute `print_max_nsources` that can be set for the `Catalogue` object;\n",
    "- provide a method `filter_by_types()` that takes as an argument a list of strings and returns a new instance of `Catalogue` containing only the sources for which the value of the `type` field is comprised in the list; the syntax of the operation should be like `new_cat = full_cat.filter_by_types([\"SN Ia\"])`;\n",
    "- provide methods `get_brightest_source()` and `get_closest_source()` where the brightest source is the one with the lower value of `peakmag` and the closest source is the one with the lower value of `redshift`; the return value should contain the data of the source in the form of a dictionary (ideally, we would like to define a dedicated `Source` class in such circumstance but this is not a requirement for this exercise);\n",
    "- provide a method `__eq__(self, other)` that takes as an argument another instance of `Catalogue` and returns `True` if the **content** of the two catalogues is the same, regardless of the `origin`. Tip: remember you can compare list contents with `==` and that you can make use of `all()` and `any()` special functions for booleans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "class Catalogue:\n",
    "    # Set here a class-wide default value, then provide a method to configure it.\n",
    "    default_print_max_nsources = 10\n",
    "\n",
    "    # We may need to use a reference field name in the methods, so we set it here.\n",
    "    id_field = \"IAUID\" \n",
    "\n",
    "    def __init__(self, input_dict : dict, origin: str = \"runtime dictionary\" ):\n",
    "        self.data = input_dict\n",
    "        self.origin = origin\n",
    "        self.print_max_nsources = Catalogue.default_print_max_nsources\n",
    "\n",
    "    def set_print_max_nsources(self, n):\n",
    "        self.print_max_nsources = n\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" \n",
    "        If `other` is a `Catalogue` (we could incur in errors otherwise) we compare the data.\n",
    "        Otherwise the comparison is just false.\n",
    "        \"\"\"\n",
    "        if isinstance(other, Catalogue):\n",
    "            return self.data == other.data\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[self.id_field])\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr = f\"Catalogue with {len(self.data.keys())} fields holding {len(self.data[self.id_field])} objects.\"\n",
    "        repr += f\"\\nOrigin: {self.origin}\"\n",
    "        return repr\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        This has to build a string that looks like a table.\n",
    "        We build a line out of the field names (header) plus a given number of lines of actual data.\n",
    "        For simplicity, we choose to convert every value to string and let it take 8 spaces (using a \":8s\" format string). It would be more elegant to define a different format string each field and better control the displaying of float values!\n",
    "        \"\"\"\n",
    "        header_str, table_str = \"\", \"\"\n",
    "        for fieldname in self.data.keys():\n",
    "            header_str += f\"| {fieldname:8s} \"\n",
    "        header_str += \"| \\n\"\n",
    "\n",
    "        for r, row in enumerate(self.data[self.id_field]):\n",
    "            if r >= self.print_max_nsources:\n",
    "                break\n",
    "            for fieldname in self.data.keys():\n",
    "                table_str += f\"| {str(self.data[fieldname][r]):8s} \"\n",
    "            table_str += \"| \\n\"\n",
    "\n",
    "        \"\"\"\n",
    "        Let's be nice and informative!\n",
    "        \"\"\"\n",
    "        hidden_entries = len(self) - r\n",
    "        if hidden_entries > 0:\n",
    "            table_str += f\"{hidden_entries} additional entries not shown.\"\n",
    "\n",
    "        return header_str + table_str\n",
    "\n",
    "    def filter_by_types(self, typelist):\n",
    "        _data = self.data # just a shorthand notation\n",
    "        filtered_dict = { key : list() for key in _data.keys() }\n",
    "        for r, row in enumerate(_data[self.id_field]):\n",
    "            if _data['type'][r] in typelist:\n",
    "                for key in _data.keys():\n",
    "                    filtered_dict[key].append(_data[key][r])\n",
    "        origin = f\"Filter by types {typelist} on Catalogue with origin:\\n - {self.origin}\"  \n",
    "        return Catalogue(filtered_dict, origin=origin)\n",
    "\n",
    "    def get_closest_source(self):\n",
    "        z = self.data['redshift']\n",
    "        i_min = z.index(min(z))\n",
    "        src = dict()\n",
    "        for key in self.data.keys():\n",
    "            src[key] = self.data[key][i_min]\n",
    "        return src\n",
    "\n",
    "    def get_brightest_source(self):\n",
    "        m = self.data['peakmag']\n",
    "        i_min = m.index(min(m))\n",
    "        src = dict()\n",
    "        for key in self.data.keys():\n",
    "            src[key] = self.data[key][i_min]\n",
    "        return src\n",
    "\n",
    "    \"\"\"\n",
    "    We can actually generalise this logic.\n",
    "    \"\"\"\n",
    "    def select_source(self, fieldname, selection):\n",
    "        d = self.data[fieldname]\n",
    "        i = d.index(selection(d))\n",
    "        src = dict()\n",
    "        for key in self.data.keys():\n",
    "            src[key] = self.data[key][i]\n",
    "        return src\n",
    "\n",
    "\n",
    "    def to_json(self, json_path):\n",
    "        \"\"\" \n",
    "        For simplicity, we do not preserve the `origin` information when dumping to JSON.\n",
    "        Otherwise we would need to build some kind of nested `origin` structure when we read back from the JSON file.\n",
    "        \"\"\"\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(self.data, f)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "        json_origin = f\"JSON file : {json_path}\"\n",
    "        return Catalogue(input_dict=json_data, origin=json_origin)\n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(cls, file_path):\n",
    "        \"\"\" This code is the same as `read_bts` \"\"\"\n",
    "        with open(file_path) as f:\n",
    "            for row in f:\n",
    "                if row.isspace():\n",
    "                    continue\n",
    "                else:\n",
    "                    fieldnames = [field.strip() for field in row.split(',')]\n",
    "                    break\n",
    "\n",
    "            data = { field : list() for field in fieldnames}\n",
    "\n",
    "            reader = csv.DictReader(f, fieldnames=fieldnames)\n",
    "            for r in reader:\n",
    "                for field in fieldnames:\n",
    "                    data[field].append(r[field])\n",
    "                    \n",
    "        numeric_fields = ['peakt', 'peakmag', 'peakabs', 'duration', 'rise', 'fade', 'redshift','b','A_V']\n",
    "\n",
    "        for field in numeric_fields:\n",
    "            column = data[field]\n",
    "            numeric_column = list()\n",
    "            for val in column:\n",
    "                if val in [\"-\", \"\"] or val.startswith(('>','<')):\n",
    "                    num_val = None\n",
    "                else:\n",
    "                    num_val = float(val)\n",
    "                numeric_column.append(num_val)\n",
    "            data[field] = numeric_column\n",
    "        \n",
    "        origin = f\"file = {file_path}\"\n",
    "\n",
    "        return Catalogue(input_dict=data, origin=origin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the two ways of building a Catalogue object!\n",
    "\"\"\"\n",
    "cat_a = Catalogue(input_dict=dataset)\n",
    "\n",
    "cat_b = Catalogue.from_csv(catalogue_path)\n",
    "\n",
    "print(cat_a.origin)\n",
    "\n",
    "print(cat_b.origin)\n",
    "\n",
    "print(\"cat_a == cat_b : \", cat_a == cat_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_c = Catalogue.from_csv(catalogue_path)\n",
    "cat_c.data[\"ZTFID\"][0] = \"INVALID\"\n",
    "\n",
    "print(\"cat_a == cat_c : \", cat_a == cat_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test __repr__()\n",
    "cat_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test __str__()\n",
    "print(cat_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tde = cat_a.filter_by_types([\"TDE\"])\n",
    "\n",
    "cat_tde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_tde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_tde.get_brightest_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_tde.get_closest_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check this alternative and more general implementation!\n",
    "print(cat_tde.select_source('redshift', min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CSV to JSON and viceversa\n",
    "- Provide the class with a method `to_json()` that takes as an argument a file path and writes a JSON representation of the object content to a file. Store the `origin` information in the output file. Remember: while a generic object cannot be directly *dumped* to JSON, a dictionary wrapping the relevant data can!\n",
    "- Provide the class with a class method `from_json()` that takes as an argument a file path and returns a `Catalogue` instance based on the file content.\n",
    "- Test that the read/write functionality works by verifying the equivalence (you have defined an `__eq__` method for this)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TEST EXAMPLE \"\"\"\n",
    "json_path = data_dir / \"bts.json\" # define your path to a file here\n",
    "\n",
    "csv_cat = Catalogue.from_csv(catalogue_path)\n",
    "csv_cat.to_json(json_path)\n",
    "json_cat = Catalogue.from_json(json_path)\n",
    "\n",
    "print(json_cat == csv_cat) # here python should invoke your `__eq__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2549115f32c44c7a8b5a7e02bed83e0a52b02d36b801495d26c408aeeebb7054"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
